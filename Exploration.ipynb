{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load training data file and competition data file\n",
    "# competition data is unseen data, for which we do not know the results (at all)\n",
    "\n",
    "# 1 = mac, 2 = pc\n",
    "platform = 1 \n",
    "trainingDataFile = \"\"\n",
    "competitionDataFile = \"\"\n",
    "\n",
    "if platform == 1:\n",
    "    trainingDataFile = \"/Abby/Resources/MLData/santander-customer-transaction-prediction/train.csv\"\n",
    "    competitionDataFile = \"/Abby/Resources/MLData/santander-customer-transaction-prediction/test.csv\"\n",
    "\n",
    "elif platform == 2:\n",
    "    trainingDataFile = \"E:\\\\Resources\\\\MLData\\\\Kaggle\\\\santander-customer-transaction-prediction\\\\train.csv\"\n",
    "    competitionDataFile = \"E:\\\\Resources\\\\MLData\\\\Kaggle\\\\santander-customer-transaction-prediction\\\\test.csv\"\n",
    "\n",
    "\n",
    "currentDirectory = os.getcwd()\n",
    "os.chdir(os.path.dirname(trainingDataFile))\n",
    "\n",
    "dfTrainingData = pd.read_csv(os.path.basename(trainingDataFile))\n",
    "dfCompetitionData = pd.read_csv(os.path.basename(competitionDataFile))\n",
    "\n",
    "os.chdir(currentDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL RUN STEP\n",
    "#\n",
    "#how many rows and columns?\n",
    "print (dfTrainingData.shape)\n",
    "print (dfCompetitionData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL RUN STEP\n",
    "#\n",
    "# sample first 5 rows of training data\n",
    "dfTrainingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL RUN STEP\n",
    "#\n",
    "# sample first 5 rows of competition data\n",
    "dfCompetitionData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the index column\n",
    "dfTrainingData = dfTrainingData.set_index('ID_code')\n",
    "dfCompetitionData = dfCompetitionData.set_index('ID_code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory usage and optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL RUN STEP\n",
    "#\n",
    "\n",
    "# memory footprint\n",
    "# memory_usage=deep ensures pandas does not provide a rough estimate of the memory usage.\n",
    "dfTrainingData.info(verbose=1,max_cols=300,memory_usage='deep')\n",
    "\n",
    "# how to read the results:\n",
    "# there are 201 columns, 200,000 rows\n",
    "# 200 rows are float64\n",
    "# 1 row is int 64\n",
    "# all 202 columns contain 200,000 values - no missing values anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL RUN STEP\n",
    "#\n",
    "dfCompetitionData.info(verbose=1,max_cols=300,memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate out the features and the target variable from dfTrainingData\n",
    "features = ['var_' + str(i) for i in range(0, 200)]\n",
    "dfTrainingFeatureVariables = dfTrainingData.loc[:, features]\n",
    "dfTrainingTargetVariable = dfTrainingData.loc[:,['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL RUN STEP\n",
    "#\n",
    "for dtype in ['float','int','object']:\n",
    "    selected_dtype = dfTrainingData.select_dtypes(include=[dtype])\n",
    "    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n",
    "    mean_usage_mb = mean_usage_b / 1024 ** 2\n",
    "    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalized and standardized copy of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMax scaler to normalize all numeric attributes in dfTrainingFeatureVariables between 0 and 1\n",
    "# note - all 200 feature variables in this case are numeric\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(dfTrainingFeatureVariables.values)\n",
    "\n",
    "# Apply transform to both the training set and the competiton set.\n",
    "# output is NumPy ndarray\n",
    "ndNormalizedTrainingFeatureVariables = scaler.transform(dfTrainingFeatureVariables.values)\n",
    "ndNormalizedCompetitionData = scaler.transform(dfCompetitionData.values)\n",
    "\n",
    "# create dataframes with normalized information\n",
    "dfNormalizedTrainingFeatureVariables = pd.DataFrame(data=ndNormalizedTrainingFeatureVariables)\n",
    "dfNormalizedTrainingFeatureVariables.columns = dfTrainingFeatureVariables.columns\n",
    "\n",
    "dfNormalizedCompetitionData = pd.DataFrame(data=ndNormalizedCompetitionData)\n",
    "dfNormalizedCompetitionData.columns = dfCompetitionData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RobustScaler scaler to normalize all numeric attributes in dfTrainingFeatureVariables between 0 and 1\n",
    "# note - all 200 feature variables in this case are numeric\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "robust_scaler.fit(dfTrainingFeatureVariables.values)\n",
    "\n",
    "# Apply transform to both the training set and the competiton set.\n",
    "# output is NumPy ndarray\n",
    "ndScaledTrainingFeatureVariables = robust_scaler.transform(dfTrainingFeatureVariables.values)\n",
    "ndScaledCompetitionData = robust_scaler.transform(dfCompetitionData.values)\n",
    "\n",
    "# create dataframes with normalized information\n",
    "dfScaledTrainingFeatureVariables = pd.DataFrame(data=ndScaledTrainingFeatureVariables)\n",
    "dfScaledTrainingFeatureVariables.columns = dfTrainingFeatureVariables.columns\n",
    "\n",
    "dfScaledCompetitionData = pd.DataFrame(data=ndScaledCompetitionData)\n",
    "dfScaledCompetitionData.columns = dfCompetitionData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection, removal, and imputation - zScore (on Scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def impute_zscore_outliers_with_median(df_input, z_score_threshold, copy):\n",
    "    # calculare zscores\n",
    "    zscores = np.abs(stats.zscore(df_input))\n",
    "    \n",
    "    # calculate median of each column\n",
    "    median_values = df_input.median()\n",
    "    \n",
    "    # make a copy of the data frame if needed\n",
    "    df_result = df_input\n",
    "    if copy == True:\n",
    "        df_result = df_input.copy(deep=True)\n",
    "    \n",
    "    # find the positions in the df where zscores are greater\n",
    "    # than the threshold\n",
    "    rows, cols = np.where(zscores > z_score_threshold)\n",
    "    \n",
    "    # replace outliera with median\n",
    "    for index in range(0, rows.size):\n",
    "        \n",
    "        ypos = rows[index]\n",
    "        xpos = cols[index]\n",
    "        \n",
    "        median = median_values[xpos]\n",
    "        df_result.iloc[ypos, xpos] = median\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "def delete_zscore_outliers(df_input, z_score_threshold):\n",
    "    # calculare zscores\n",
    "    zscores = np.abs(stats.zscore(df_input))\n",
    "    \n",
    "    # acceptable_zscores_scaled is an array of booleans, of the same dimensions as zscores_scaled.\n",
    "    acceptable_zscores = (zscores < z_score_threshold)\n",
    "\n",
    "    # (zscores < N).all(axis=1) with return a one dimensional array, each element in this array \n",
    "    # will be true if all the elements in the corresponding row of (zscores < N) are true\n",
    "    row_indexes = acceptable_zscores.all(axis=1)\n",
    "    \n",
    "    return df_input[row_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import stats\n",
    "\n",
    "# zscores_scaled is a 2d array of the same dimensions as dfScaledTrainingFeatureVariables\n",
    "# each element in zscores_scaled is the zscore.\n",
    "#zscores_scaled = np.abs(stats.zscore(dfScaledTrainingFeatureVariables))\n",
    "\n",
    "# acceptable_zscores_scaled is an array of booleans, of the same dimensions as zscores_scaled.\n",
    "#acceptable_zscores_scaled = (zscores_scaled < 3)\n",
    "\n",
    "# (zscores_scaled < 3).all(axis=1) with return a one dimensional array, each element in this array \n",
    "# will be true if all the elements in the corresponding row of (zscores_scaled < 3) are true\n",
    "#row_indexes_by_zscore = acceptable_zscores_scaled.all(axis=1)\n",
    "\n",
    "#dfScaledTrainingFeatureVariables_WithoutOutliers = dfScaledTrainingFeatureVariables[row_indexes_by_zscore]\n",
    "\n",
    "dfScaledTrainingFeatureVariables_WithoutOutliers = delete_zscore_outliers(dfScaledTrainingFeatureVariables, 3)\n",
    "dfScaledTrainingFeaturesVariables_ImputedOutliers = impute_zscore_outliers_with_median(dfScaledTrainingFeatureVariables, 3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier detection, removal, and imputation IQR (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def impute_iqr_outliers_with_median(df_input, copy):\n",
    "    # calculare IQR\n",
    "    Q1 = df_input.quantile(0.25)\n",
    "    Q3 = df_input.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # compute min and max acceptable values for each variable\n",
    "    MIN = (Q1 - 1.5 * IQR)\n",
    "    MAX = (Q3 + 1.5 * IQR)\n",
    "    \n",
    "    # calculate median of each column\n",
    "    median_values = df_input.median()\n",
    "    \n",
    "    # make a copy of the data frame if needed\n",
    "    df_result = df_input\n",
    "    if copy == True:\n",
    "        df_result = df_input.copy(deep=True)\n",
    "    \n",
    "    # find the positions in the df where values are outside the IQR\n",
    "    rows, cols = np.where((df_input < MIN) | (df_input > MAX))\n",
    "    \n",
    "    # replace outliera with median\n",
    "    for index in range(0, rows.size):\n",
    "        \n",
    "        ypos = rows[index]\n",
    "        xpos = cols[index]\n",
    "        \n",
    "        median = median_values[xpos]\n",
    "        df_result.iloc[ypos, xpos] = median\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "\n",
    "def delete_iqr_outliers(df_input):\n",
    "    # calculare IQR\n",
    "    Q1 = df_input.quantile(0.25)\n",
    "    Q3 = df_input.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # compute min and max acceptable values for each variable\n",
    "    MIN = (Q1 - 1.5 * IQR)\n",
    "    MAX = (Q3 + 1.5 * IQR)\n",
    "\n",
    "    return df_input[~((df_input < MIN) | (df_input > MAX)).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1_NormalizedTrainingFeatureVariables = dfNormalizedTrainingFeatureVariables.quantile(0.25)\n",
    "#Q3_NormalizedTrainingFeatureVariables = dfNormalizedTrainingFeatureVariables.quantile(0.75)\n",
    "#IQR_NormalizedTrainingFeatureVariables = Q3_NormalizedTrainingFeatureVariables - Q1_NormalizedTrainingFeatureVariables\n",
    "\n",
    "#MIN_NormalizedTrainingFeatureVariables = (Q1_NormalizedTrainingFeatureVariables - 1.5 * IQR_NormalizedTrainingFeatureVariables)\n",
    "#MAX_NormalizedTrainingFeatureVariables = (Q3_NormalizedTrainingFeatureVariables + 1.5 * IQR_NormalizedTrainingFeatureVariables)\n",
    "\n",
    "#dfNormalizedTrainingFeatureVariables_WithoutOutliers = dfNormalizedTrainingFeatureVariables[~((dfNormalizedTrainingFeatureVariables < MIN_NormalizedTrainingFeatureVariables) | (dfNormalizedTrainingFeatureVariables > MAX_NormalizedTrainingFeatureVariables)).any(axis=1)]\n",
    "\n",
    "dfNormalizedTrainingFeatureVariables_WithoutOutliers = delete_iqr_outliers(dfNormalizedTrainingFeatureVariables)\n",
    "dfNormalizedTrainingFeatureVariables_ImputedOutliers = impute_iqr_outliers_with_median(dfNormalizedTrainingFeatureVariables, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24896"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRowsDropped = dfNormalizedTrainingFeatureVariables.shape[0] -  dfNormalizedTrainingFeatureVariables_WithoutOutliers.shape[0]\n",
    "numRowsDropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRowsDropped = dfNormalizedTrainingFeatureVariables.shape[0] -  dfNormalizedTrainingFeatureVariables_ImputedOutliers.shape[0]\n",
    "numRowsDropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "dataset = load_iris()\n",
    "\n",
    "df_iris_features = pd.DataFrame(data = dataset.data, columns=dataset.feature_names)\n",
    "\n",
    "# calculare zscores\n",
    "zscores_iris = np.abs(stats.zscore(df_iris_features))\n",
    "\n",
    "# delete all rows with zscores > 2\n",
    "iris_data_without_outliers = df_iris_features[(zscores_iris < 2).all(axis=1)]\n",
    "\n",
    "# impute all values with zscore > 2 with column median\n",
    "df_iris_features_with_imputed_outliers = impute_outliers_with_median(df_iris_features, 2)\n",
    "df_iris_features_with_imputed_outliers = df_iris_features.copy(deep=True)\n",
    "median_values = df_iris_features.median()\n",
    "\n",
    "rows, cols = np.where(zscores_iris > 2)\n",
    "for index in range(0, rows.size):\n",
    "        \n",
    "    ypos = rows[index]\n",
    "    xpos = cols[index]\n",
    "        \n",
    "    median = median_values[xpos]\n",
    "    df_iris_features_with_imputed_outliers.iloc[ypos, xpos] = median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL RUN STEP\n",
    "#\n",
    "\n",
    "\n",
    "# summary statistics of training data (withoout the target)\n",
    "# note - unique, top, and frequency are only available for categorical data\n",
    "dfSummary1 = dfNormalizedTrainingFeatureVariables.describe(include='all')\n",
    "\n",
    "dfSummary1.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL RUN STEP\n",
    "#\n",
    "\n",
    "# inspect characteristics of ndarray: dfNormalizedCompetitionData\n",
    "dfNormalizedCompetitionData = pd.DataFrame(data=ndNormalizedCompetitionData)\n",
    "\n",
    "# summary statistics of training data (withoout the target)\n",
    "# note - unique, top, and frequency are only available for categorical data\n",
    "dfSummary2 = dfNormalizedCompetitionData.describe(include='all')\n",
    "\n",
    "dfSummary2.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "=====================================================\n",
    "==== PCA dimensionality reduction  ====\n",
    "====================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95 - 50%  PCA \n",
    "from sklearn.decomposition import PCA\n",
    "pca95 = PCA(.95)\n",
    "pca90 = PCA(.90)\n",
    "pca85 = PCA(.85)\n",
    "pca80 = PCA(.80)\n",
    "pca75 = PCA(.75)\n",
    "pca70 = PCA(.70)\n",
    "pca65 = PCA(.65)\n",
    "pca60 = PCA(.60)\n",
    "pca55 = PCA(.55)\n",
    "pca50 = PCA(.50)\n",
    "\n",
    "ndPCA95_StandardizedTrainingFeatureVariables = pca95.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA90_StandardizedTrainingFeatureVariables = pca90.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA85_StandardizedTrainingFeatureVariables = pca85.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA80_StandardizedTrainingFeatureVariables = pca80.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA75_StandardizedTrainingFeatureVariables = pca75.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA70_StandardizedTrainingFeatureVariables = pca70.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA65_StandardizedTrainingFeatureVariables = pca65.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA60_StandardizedTrainingFeatureVariables = pca60.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA55_StandardizedTrainingFeatureVariables = pca55.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "ndPCA50_StandardizedTrainingFeatureVariables = pca50.fit_transform(ndStandardizedTrainingFeatureVariables)\n",
    "\n",
    "\n",
    "ndPCA95_StandardizedCompetitionData = pca95.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA90_StandardizedCompetitionData = pca90.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA85_StandardizedCompetitionData = pca85.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA80_StandardizedCompetitionData = pca80.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA75_StandardizedCompetitionData = pca75.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA70_StandardizedCompetitionData = pca70.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA65_StandardizedCompetitionData = pca65.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA60_StandardizedCompetitionData = pca60.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA55_StandardizedCompetitionData = pca55.fit_transform(ndStandardizedCompetitionData)\n",
    "ndPCA50_StandardizedCompetitionData = pca50.fit_transform(ndStandardizedCompetitionData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (ndPCA95_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA90_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA85_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA80_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA75_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA70_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA65_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA60_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA55_StandardizedTrainingFeatureVariables.shape)\n",
    "print (ndPCA50_StandardizedTrainingFeatureVariables.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (ndPCA95_StandardizedCompetitionData.shape)\n",
    "print (ndPCA90_StandardizedCompetitionData.shape)\n",
    "print (ndPCA85_StandardizedCompetitionData.shape)\n",
    "print (ndPCA80_StandardizedCompetitionData.shape)\n",
    "print (ndPCA75_StandardizedCompetitionData.shape)\n",
    "print (ndPCA70_StandardizedCompetitionData.shape)\n",
    "print (ndPCA65_StandardizedCompetitionData.shape)\n",
    "print (ndPCA60_StandardizedCompetitionData.shape)\n",
    "print (ndPCA55_StandardizedCompetitionData.shape)\n",
    "print (ndPCA50_StandardizedCompetitionData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split on non-pca decomponsed dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dfStandardizedTrainingData, dfTrainingTargetVariable, test_size=0.25, random_state=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regresion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(x_train, y_train)\n",
    "\n",
    "predictions = logisticRegr.predict(x_test)\n",
    "\n",
    "# compute accuracy\n",
    "#accuracy = logisticRegr.score(x_train, x_test)\n",
    "#print (accuracy)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(x_test, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc)\n",
    "#print(test_performance.auc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
